{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain the best model from a random search done with classifier_train.py\n",
    "First load the results from the search and check wich configuration performed best.\n",
    "Then retrain with some learning rate decay and more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pal = sns.color_palette(\"colorblind\")\n",
    "plt.style.use('plot_style.txt')\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('/mnt/md0/aholmberg/data/raytracing_class_random_25_spherical.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the features in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(df['n_sol'])\n",
    "sc_pos_d = df['source_pos_d'].to_numpy().astype(np.float32)\n",
    "sc_pos_phi = df['source_pos_phi'].to_numpy().astype(np.float32)\n",
    "ant_pos_z = df['antenna_pos_z'].to_numpy().astype(np.float32)\n",
    "features = np.stack((sc_pos_d, sc_pos_phi, ant_pos_z), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize and split into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features  = features.astype(np.float32)\n",
    "labels[labels == 2] = 1\n",
    "\n",
    "norm_x = np.zeros_like(features)\n",
    "norm_x[:, 0] = features[:, 0] / (np.sqrt(2700**2 + 2000**2))\n",
    "norm_x[:, 1] = features[:, 1] / (180)\n",
    "norm_x[:, 2] = features[:, 2] / -(200)\n",
    "\n",
    "norm_features_train = norm_x[:int(norm_x.shape[0]*0.8)]\n",
    "norm_features_test = norm_x[int(norm_x.shape[0]*0.8):]\n",
    "labels_train = labels[:int(labels.shape[0]*0.8)]\n",
    "labels_test = labels[int(labels.shape[0]*0.8):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the results from the run and then print the five best and five worst models in latex formated table\n",
    "putting the to_latex line in a print statement gives a reslut that you can vopy to a latex file or save it in a file directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = pd.read_pickle(f'class-run-1.pkl')\n",
    "table = pd.concat([models.sort_values('val_acc', ascending=False).iloc[:5], models.sort_values('val_acc', ascending=False).iloc[-5:]], ignore_index=True, axis=0)\n",
    "table.to_latex(columns=['lr', 'depth', 'width', 'act', 'val_acc'], )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the classification model \n",
    "Should be done by loading the model from the saved models instead of doing it manually like now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifier(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=False)):\n",
    "    inputs = layers.Input(shape=(3,))\n",
    "    x = layers.Dense(48, activation='relu', kernel_initializer='he_normal')(inputs)\n",
    "    x = layers.Dense(48, activation='relu', kernel_initializer='he_normal')(x)\n",
    "    x = layers.Dense(48, activation='relu', kernel_initializer='he_normal')(x)\n",
    "    x = layers.Dense(48, activation='relu', kernel_initializer='he_normal')(x)\n",
    "    x = layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = keras.Model(inputs, x)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=['accuracy', keras.metrics.TruePositives(), keras.metrics.TrueNegatives(), keras.metrics.FalsePositives(), keras.metrics.FalseNegatives()]\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_classifier()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the callbacks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch <= 0:\n",
    "        return 1e-2\n",
    "    elif epoch == 1:\n",
    "        return 1e-3\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "checkpoint_filepath = '/mnt/md0/aholmberg/class_models/check/' + 'class_test'\n",
    "if not os.path.isdir(checkpoint_filepath):\n",
    "    os.mkdir(checkpoint_filepath)\n",
    "\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with a validation split of 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(norm_features_train,\n",
    "          labels_train,\n",
    "          epochs=20,\n",
    "          batch_size=512,\n",
    "          validation_split=0.1,\n",
    "          callbacks=[model_checkpoint_callback, lr_scheduler],\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the best epoch and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model.evaluate(norm_features_test, labels_test, batch_size=2048)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the data for confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total numbers\n",
    "plot_data = np.array([[y[2], y[5]],\n",
    "                      [y[4], y[3]]])\n",
    "\n",
    "# rates\n",
    "#plot_data = np.array([[y[2]/(y[2] + y[5]), y[5]/(y[5] + y[2])],\n",
    "#                      [y[4]/(y[4] + y[3]), y[3]/(y[3] + y[4])]])\n",
    "\n",
    "plot_data = pd.DataFrame(data=plot_data, index=['Solution', 'No solution'], columns=['Solution', 'No solution'])\n",
    "\n",
    "# rates\n",
    "#labels = np.array([[f'{y[2]/(y[2] + y[5]):.4f}', f'{y[5]/(y[5] + y[2]):.4f}'],\n",
    "#                   [f'{y[4]/(y[4] + y[3]):.4f}', f'{y[3]/(y[3] + y[4]):.4f}']])\n",
    "\n",
    "# total numbers\n",
    "labels = np.array([[f'{y[2]:.2E}', f'{y[5]:.2E}'],\n",
    "                   [f'{y[4]:.2E}', f'{y[3]:.2E}']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(8,5))\n",
    "sns.heatmap(plot_data, annot=labels, fmt='', cmap='Blues')\n",
    "ax.set_xlabel('Preticted value')\n",
    "ax.set_ylabel('True value')\n",
    "#fig.savefig('thesis/Exjobb-rapport/figures/confusion-matrix.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the raw prediction of the model and save in a dataframe to use with seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = model.predict(norm_features_test, batch_size=4096)\n",
    "hist_data = pd.DataFrame(np.stack((ys.squeeze(), labels_test), axis=-1), columns=['Prediction', 'True'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8,5))\n",
    "ax = sns.histplot(hist_data, x='Prediction', hue='True', log_scale=(False,True), legend=False, palette=pal[:2])\n",
    "ax.legend(labels=['Solution', 'No solution'], frameon=False)\n",
    "ax.set_xlim(0,1)\n",
    "ax.xaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "#fig.savefig('thesis/Exjobb-rapport/figures/class_hist.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, \"solution_classifier.png\", rankdir='hR', show_shapes=True, show_layer_names=False, dpi=300)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
